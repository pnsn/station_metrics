#!/usr/bin/env python

# Calculate some station metrics - noise floor, power at different frequencies, Nspikes...


# Generate a lock file in /tmp so that only one instance of eew_stationreport can run.

import os
import sys

pid = str(os.getpid())
pidfile = "/tmp/eew_stationreport.lockfile_Mouse"

if os.path.isfile(pidfile):
    print ("%s exists.  Exiting." % pidfile)
    sys.exit()
else:
    file = open(pidfile, 'w')
    file.write(pid)
    file.close()

# import std packages
from collections import Counter
import datetime
from datetime import timedelta
import glob
import timeit
import fileinput
from collections import OrderedDict

# import third party packages
import numpy as np
from obspy.clients.fdsn import Client
from obspy import UTCDateTime
from obspy.signal.trigger import classic_sta_lta
from obspy import Stream
#try:
#    from ConfigParser import SafeConfigParser
#except:
#    from configparser import SafeConfigParser
try:
    from ConfigParser import ConfigParser
except:
    from configparser import ConfigParser

from jinja2 import Environment, FileSystemLoader, select_autoescape
from weasyprint import HTML

# import station_metrics packages
from station_metrics.io.get_data_metadata import download_fdsn_bulk
from station_metrics.io.get_data_metadata import download_metadata_fdsn
from station_metrics.io.external import latency_gaps_completeness

from station_metrics.metrics.preprocessing import raw_trace_to_ground_motion_filtered_pruned
from station_metrics.metrics.noise_metrics import noise_floor, count_peaks_stalta
from station_metrics.metrics.noise_metrics import duration_exceed_RMS

from station_metrics.plotting.plot_eew_stationreport import make_station_figure_eew_stationreport

from config.parse_and_validate_args import validate_args_and_get_times, parse_args

# define how many days we want latency info for (at most)
MAX_LATENCY_DAYS = 14


# define which channel codes denote acceptable seismic channels
SEISMIC_CHANNELS = ['BHE', 'BHN', 'BHZ', 'HHE', 'HHN', 'HHZ',
                    'BH1', 'BH2', 'BH3', 'HH1', 'HH2', 'HH3',
                    'ENE', 'ENN', 'ENZ', 'HNE', 'HNN', 'HNZ',
                    'EN1', 'EN2', 'EN3', 'HN1', 'HN2', 'HN3']
# define a few SOH channels we might be interested in
SOH_CHANNELS = ['LCQ', 'LCE']

THRESHOLD = { 
             'lcq':60,               # minimum quality=60, read Q330 manual
             'lce':5000,             # maximum phase(drift)=5000 microseconds
             'percent_latency_good':98,          # good=maximum latency 3.5s between now and middle of packet
             'percent_completeness':90,          
             'gaps_per_hour':1,	     # maximum number of gaps per hour
             'rms_noise':0.0007,     # maximum RMS amplitude 0.07 cm/s^2 
             'spikes' : 0.0034       # maximum spike amplitude 0.34 sm/s^2
            }

if __name__ == "__main__":
    """
        Generates Phase1 Station Acceptance Report.
        Requirements:
            config/config.eew_stationreport --> configuring the metrics
            command-line arguments --> specify station and time period
    """
    # prepare report engine (jinja2)
    env = Environment(
        loader=FileSystemLoader("./templates"),
        autoescape=select_autoescape(['html', 'htm', 'xml'])
    )
    template = env.get_template('phase1.htm')
        
    #----- Read Processing parameters from a config file
    
#    parser = SafeConfigParser()
    parser = ConfigParser()
    parser.read("config/config.eew_stationreport")
    
    FDSNtimeout = float(parser.get('SectionOne','FDSNtimeout'))
    tpadding = float(parser.get('SectionOne','tpadding'))
    sta = float(parser.get('SectionOne','sta'))
    lta = float(parser.get('SectionOne','lta'))
    tbuffer = sta + lta
    freqBP1 = float(parser.get('SectionOne','freqBP1'))
    freqBP2 = float(parser.get('SectionOne','freqBP2'))
    freqBP3 = float(parser.get('SectionOne','freqBP3'))
    freqBP4 = float(parser.get('SectionOne','freqBP4'))
    freqHP = float(parser.get('SectionOne','freqHP'))
    mpd = float(parser.get('SectionOne','mpd'))
    minSTALTAlen = float(parser.get('SectionOne','minSTALTAlen'))
    RMSlen = float(parser.get('SectionOne','RMSlen'))
    GainOrFullResp = parser.get('SectionOne','GainOrFullResp')
    sniffwave_tally_duration = int(parser.get('SectionOne','sniffwave_tally_duration'))
    #PSDperiods = (parser.get('SectionOne','PSDperiods'))[1:-1].split(",")
    #PSDperiods = [float(i) for i in PSDperiods]

    #----- command-line arguments specify what data to get and where
    # options that influence program structure:
    # infile: 
    #     multiple channels, possibly from multiple stations, are defined in an input file. 
    # command-line arguments -N network, -S station:
    #     eew_stationreport is run for the channels of a single station only.
    
    args = parse_args()
    [starttime, endtime, durationinhours, durationinsec] = validate_args_and_get_times(args)
    starttime = starttime.replace(microsecond=0)
    endtime = endtime.replace(microsecond=0)
    nowtime = datetime.datetime.now()
    DateLabel = str(nowtime.strftime('v%Y-%m-%dT%H-%M'))

    # get data from a little before starttime to remedy edge effects
    request_starttime = starttime - datetime.timedelta(0,lta+tpadding)
    duration_padded = durationinsec + sta + lta + (2*tpadding)
    request_endtime = starttime + datetime.timedelta(seconds=duration_padded)

    slice_starttime = starttime - datetime.timedelta(seconds=lta)
    slice_endtime = starttime + datetime.timedelta(seconds=(durationinsec + sta))

    endminusstarttime = (endtime - starttime).total_seconds()
    if ( endminusstarttime < minSTALTAlen ):
        print ("EXITING: requested time is too short or wrong " + str(starttime) + " to " + str(endtime) )
        sys.exit()

    # find out where output reports go
    if not "REPORT_DIR" in os.environ:
        report_dir = "./"

#    save for later, not needed now (decided to dump it all into report_dir)
#    # create report directories if they don't exist yet
#    if not os.path.exists(report_dir+"/html"):
#        try:
#            os.makedirs(report_dir + "/html")
#        except Exception as e:
#            print("Error creating output directory: {}".format(e))
#            sys.exit(1)
#
#    if not os.path.exists(report_dir+"/pdf"):
#        try:
#            os.makedirs(report_dir + "/pdf")
#        except Exception as e:
#            print("Error creating output directory: {}".format(e))
#            sys.exit(1)

    if not os.path.isdir(report_dir):
        print("Error: {} exists but does not appear to be a directory")
        sys.exit(1)

    infile = args.infile
    if infile:
        network = None
        station = None
    else: 
        network = args.network.upper()
        station = args.station.upper()
    datacenter = args.datacenter.upper()
    institution = args.institution
    email = args.email
    iplot = args.iplot
    latency_dir = args.lat_dir
    filelist = []
    if latency_dir:
        # make sure this exists and is readable
        startday = starttime.replace(hour=0,minute=0,second=0)
        if ( starttime.hour == 0 and starttime.minute <= np.ceil(sniffwave_tally_duration/60) + 1 ):
            startday = startday - timedelta(days=1) 
        endday = endtime.replace(hour=0,minute=0,second=0)
        try:
            # assume files are named YYYY_MM_DD_*.csv
            filelistall = []
            if ( institution is None ):
                filelistall = glob.glob(latency_dir + "/*tally.csv")
            else:
                filelistall = glob.glob(latency_dir + "/*tally." + institution + ".csv")
#                filelistall.extend(glob.glob(latency_dir + "/*tally." + institution.upper() + ".csv"))
#                filelistall.extend(glob.glob(latency_dir + "/*tally." + institution.lower() + ".csv"))
            # only read sniffwave_tally files in the timerange being measured

            for ifile in range(0,len(filelistall)):
               filedate = datetime.datetime.strptime(filelistall[ifile].split("/")[-1].split("_")[0], '%Y-%m-%d')
               if ( filedate >= startday and filedate <= endday ):
                   filelist.append(filelistall[ifile])
            filelist = sorted(filelist)
        except Exception as e:
            print("Error creating list of sniffwave_tally output files: {}".format(e))
    if len(filelist) > 0:
        all_latency_metrics = latency_gaps_completeness(filelist,starttime,endtime) 
    else:
        all_latency_metrics = None

    #----- Set up the FDSN client
    Tdc0 = timeit.default_timer()
    try:
        client = Client(datacenter,timeout = FDSNtimeout)
    except Exception as error:
        print ("Error: {}, Failed to connect to FDSN client. Used a timeout of {}".format(error, FDSNtimeout))
    Tdc1 = timeit.default_timer()
    print ("Time to connect to FDSNWS: " + str(Tdc1-Tdc0) )
    
    #----- Download meta-data
    timer_start = timeit.default_timer()
    if infile:
        inv = download_fdsn_bulk(infile,request_starttime,duration_padded,client,type="metadata")
    else:
        inv = download_metadata_fdsn(client,net=network,sta=station,starttime=request_starttime)
    timer_end = timeit.default_timer()
    print(inv)
    inv.write("inventory.xml","STATIONXML")
    print ("Time to download metadata from {}: {}".format(datacenter, str(timer_end - timer_start)))

    # Create one report per station in the Inventory file.
    for net_obj in inv:
        network = net_obj.code
        for sta_obj in net_obj:
            station = sta_obj.code
            bulkrequest = []
            soh_bulkrequest = []
    
            # Download waveform for this station 
    
            # construct the bulkrequest list of tuples
            # single network, single station, loop over channels
            for chan in sta_obj:
                channel = chan.code
                location = chan.location_code
                if location == "":
                    location = "--"
                if channel in SEISMIC_CHANNELS:
                    if ( datacenter == "SCEDC" and duration_padded > 691200 ):
                        request_starttime2 = request_starttime + datetime.timedelta(seconds=(duration_padded/2.))
                        request_endtime1 = request_starttime2 + datetime.timedelta(seconds=1)
                        bulkrequest1.append( \
                            (network, station, location, channel, request_starttime.isoformat(), \
                            request_endtime1.isoformat()) \
                            )
                        bulkrequest2.append( \
                            (network, station, location, channel, request_starttime2.isoformat(), \
                            request_endtime.isoformat()) \
                            )
                    else:
                        bulkrequest.append( \
                            (network, station, location, channel, request_starttime.isoformat(), \
                            request_endtime.isoformat()) \
                            )
                if channel in SOH_CHANNELS:
                    soh_bulkrequest.append( \
                        (network, station, location, channel, request_starttime.isoformat(), \
                        request_endtime.isoformat()) \
                        )

            timer_start = timeit.default_timer()
            try:
                if ( datacenter == "SCEDC" and duration_padded > 691200 ):
                    stAll1 = client.get_waveforms_bulk(bulkrequest1)
                    time.sleep(5)
                    stAll2 = client.get_waveforms_bulk(bulkrequest2)
                    stAll = stAll1 + stAll2
                    stAll.merge(method=-1)
                else:
                    stAll = client.get_waveforms_bulk(bulkrequest)
            except Exception as e:
                print("Error: Problem requesting waveform data: {}".format(e))
                # create empty stream
                stAll = Stream()
            timer_end = timeit.default_timer()
#            print ("Time to download waveforms from {}: {}".format(datacenter, str(timer_end - timer_start)))
            print ("Time to download waveforms from {}: {}".format(datacenter, str(timer_end - timer_start)) + "  Nstreams: " + str(len(stAll))  + "  " + str(stAll[0].stats.npts) + " points in first trace" )

            # Now request the SOH channels if so desired.
            soh_metrics = {}
            if len(soh_bulkrequest) == 0:
                print("No SOH channels requested")
                stSOH = Stream()
            else:
                timer_start = timeit.default_timer()
                try:
                    stSOH = client.get_waveforms_bulk(soh_bulkrequest)
                except Exception as e:
                    print("Error: Problem requesting SOH data: {}".format(e))
                    # create empty stream
                    stSOH = Stream()
     
                timer_end = timeit.default_timer()
                print ("Time to download SOH time series from {}: {}".format(datacenter, str(timer_end - timer_start)))
            
            print("Creating report for time period {} - {}, station: {}.{}".format(starttime,endtime,net_obj.code,sta_obj.code))
            # if there channels in stSOH then calculate the metric
            lcq_pass = 0
            lce_pass = 0
            number_of_lcq_points = 0
            number_of_lce_points = 0

            # anticipate possible gaps in these data but do not keep track
            for trace in stSOH:
                location = trace.stats.location
                if location == "":
                    location = "--"
                nslc = ".".join([trace.stats.network, trace.stats.station, location, trace.stats.channel])
                if trace.stats.channel == "LCQ":
                    # LCQ channel has distinct values between 0-100, e.g. 20, 60, 100
                    lcq_pass = lcq_pass + (trace.data >= THRESHOLD['lcq']).sum()
                    number_of_lcq_points = number_of_lcq_points + len(trace.data)
                if trace.stats.channel == "LCE":
                    # LCE channel can have any value, loop over all samples
                    lce_pass = lce_pass + (abs(trace.data) < THRESHOLD['lce']).sum()
                    number_of_lce_points = number_of_lce_points + len(trace.data)

            if number_of_lcq_points > 0:
                lcq_metric = 100*(lcq_pass/number_of_lcq_points)
                soh_metrics["LCQ"] = { "clock_lock_lcq" : 
                                       (lcq_metric, "pass" if lcq_metric >= 98.0 else "fail")
                                     } 
            else:
                lcq_metric = 0
            print("Number of LCQ samples at 60 or better: {} out of {}".format(lcq_pass, \
                   number_of_lcq_points))
            if number_of_lce_points > 0:
                lce_metric = 100*(lce_pass/number_of_lce_points)
                soh_metrics["LCE"] = { "clock_phase_lce" : 
                                      (lce_metric, "pass" if lce_metric >= 98.0 else "fail")
                                     } 
            else:
                lce_metric = 0
            print("Number of LCE samples below 5000 microsecs: {} out of {}".format(lce_pass, \
                   number_of_lce_points))


            #----- Loop over seismograms.  If a station has gaps, a stream of that nslc will 
            #----- be made from all the traces.
            
            # This assumes the data are in the stream ordered by net,sta,loc,chan,starttime, 
            # which is OK for the DMC (per Chad) but is NOT guaranteed by the FDSN dataselect 
            # web service standard. 
            channel_metrics = OrderedDict()
            itrace = 0
            Tsum = 0
            for i in range(0,len(stAll)):
                ntr = 1
                iduplicate = 0
                if ( stAll[i].stats.location == "" ):
                    nslc = stAll[i].stats.network + "." + stAll[i].stats.station + ".--." + stAll[i].stats.channel
                else:
                    nslc = stAll[i].stats.network + "." + stAll[i].stats.station + "." + stAll[i].stats.location + "." + stAll[i].stats.channel
            #    print ( str(i) + " SNCL " + nslc )
                for j in range(0,len(stAll)):
                    if ( stAll[j].stats.location == "" ):
                        nslc2 = stAll[j].stats.network + "." + stAll[j].stats.station + ".--." + stAll[j].stats.channel
                    else:
                        nslc2 = stAll[j].stats.network + "." + stAll[j].stats.station + "." + stAll[j].stats.location + "." + stAll[j].stats.channel
                    if ( nslc == nslc2 and i != j ):
                        if ( j < i ):
                            iduplicate = 1
                            break
                        else:
                            ntr = ntr + 1
                if ( iduplicate == 0 ):
                    T2 = timeit.default_timer()
                    itrace = itrace + 1
                    tr = []
                    trRaw = []
                    trVelHighPass = []
                    trAccFilt = []
                    trVelFilt = []
                    stalta = []
                    dataraw = []
                    dataVelHighPass = []
                    dataAccFilt = []
                    dataVelFilt = []
                    datastalta = []
                    nptstotal = 0
                    dt = stAll[i].stats.delta
                    segmentlong = 0
                    segmentshort = 1e10
                    for j in range(i,i+ntr):
                        trRaw = stAll[j].copy()
                        trRaw = trRaw.slice(UTCDateTime(slice_starttime),UTCDateTime(slice_endtime))
                        npts = trRaw.stats.npts
                        trPadded = stAll[j].copy()
                        trPadded = trPadded.detrend(type='polynomial',order=3)

                        trVelHighPass = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                     slice_starttime, slice_endtime, "Vel", 0, 0, \
                                     freqHP, 0, 0, inv)
                        trAccFilt = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                    slice_starttime, slice_endtime, "Acc", 0, freqBP1, \
                                    freqBP2, freqBP3, freqBP4, inv)
                        trVelFilt = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                    slice_starttime, slice_endtime, "Vel", 0, freqBP1, \
                                    freqBP2, freqBP3, freqBP4, inv)
                        stalta = trVelHighPass.copy()
                        dataraw = np.concatenate((dataraw,trRaw.data),axis=0)
                        dataVelHighPass = np.concatenate((dataVelHighPass,trVelHighPass.data),axis=0)
                        dataAccFilt = np.concatenate((dataAccFilt,trAccFilt.data),axis=0)
                        dataVelFilt = np.concatenate((dataVelFilt,trVelFilt.data),axis=0)

                        if ( npts*dt > minSTALTAlen ):
            #                stalta = z_detect(trVelHighPass,int(1.0/dt))                      #--- of order 0.1 sec/trace 
                            stalta = classic_sta_lta(trVelHighPass,int(sta/dt),int(lta/dt))  #--- of order 0.4 sec/trace
                            datastalta = np.concatenate((datastalta,stalta),axis=0)
                        else:
                            datastalta = np.concatenate((datastalta,np.zeros(trVelHighPass.stats.npts)),axis=0)
                        T4 = timeit.default_timer()
                        if ( iplot == 1 ):
                            if ( j == i ):
                                strawplot = Stream(traces=[trRaw])
                                stVelHighPassplot = Stream(traces=[trVelHighPass])
                                stAccFiltplot = Stream(traces=[trAccFilt])
                            else:
                                strawplot += Stream(traces=[trRaw])
                                stVelHighPassplot += Stream(traces=[trVelHighPass])
                                stAccFiltplot += Stream(traces=[trAccFilt])
                            if ( j == i + ntr -1 ):
                                AccRMSlabel = str(freqBP2) + "-" + str(freqBP3) + "Hz RMS Acc (cm/s^2)"
                                AccNspikeslabel =  str(freqBP2) + "-" + str(freqBP3) + "Hz Spikes Acc (cm/s^2)"
                                VelHighPasslabel =  "   HighPass " + str(freqHP) + "Hz    Vel (cm/s)"
                                for ij in range(0,len(stVelHighPassplot)):
                                    stVelHighPassplot[ij].data = stVelHighPassplot[ij].data*100
                                    stAccFiltplot[ij].data = stAccFiltplot[ij].data*100
#                                    stVelFiltplot[ij].data = stVelFiltplot[ij].data*100
                                make_station_figure_eew_stationreport(strawplot,stVelHighPassplot,stAccFiltplot,stAccFiltplot,datastalta,"Raw       (counts)",VelHighPasslabel,AccRMSlabel,AccNspikeslabel,"     STA/LTA",0,0,0.07,0.34,20,RMSlen)  #--- of order 25sec/channel for 1 hr long traces
                                stVelHighPassplot.clear()
                                stAccFiltplot.clear()
                        T5 = timeit.default_timer()
                        Tplot = T5-T4
                        trRaw = trRaw.slice(UTCDateTime(starttime),UTCDateTime(endtime))
                        npts = trRaw.stats.npts
                        nptstotal = nptstotal + npts
                        if ( dt*npts > segmentlong ):
                            segmentlong = dt*npts
                        if ( dt*npts < segmentshort ):
                            segmentshort = dt*npts

                    rawrange = float(np.ptp(dataraw))
                    rawmean = np.mean(dataraw)
            #        rawrms = np.sqrt(np.vdot(dataraw, dataraw)/dataraw.size)
                    rawrms = np.sqrt(np.mean((dataraw-rawmean)**2))
                    rawmin = min(dataraw)
                    rawmax = max(dataraw)
                    pctavailable = 100.*(nptstotal/(endminusstarttime/dt))
                    ngaps = ntr - 1
                    NoiseFloorAcc = noise_floor(dataAccFilt)
                    NoiseFloorVel = noise_floor(dataVelFilt)
                    dataVelFilt = []
                    snr20_0p34cm = count_peaks_stalta(dataAccFilt,datastalta,sta,lta,mpd,20,dt,THRESHOLD['spikes'])  #--- ShakeAlert stat. acceptance thresh. is now 0.0034 m/s^2 = 0.34cm/s^2.
                    datastalta = []
                    RMSduration_0p07cm = duration_exceed_RMS(dataAccFilt,THRESHOLD['rms_noise'],RMSlen,dt)  #--- ShakeAlert stat. acceptance thresh. is now 0.0007 m/s^2 = 0.07cm/s^2.
                    dataAccFilt = []
                    T3 = timeit.default_timer()
            
                    if ( ngaps/durationinhours < 1.0 ):
                        strngap = "ngaps_PASS: " + str(ngaps/durationinhours)
                        strngap = str( "ngaps_PASS: %.3f " % (ngaps/durationinhours) )
                    else:
                        strngap = "ngaps_FAIL: " + str(ngaps/durationinhours)
                        strngap = str( "ngaps_FAIL: %.3f " % (ngaps/durationinhours) )
            
                    if ( RMSduration_0p07cm/durationinhours < 60. ):
                        strrms = "rms_PASS: " + str(RMSduration_0p07cm/durationinhours)
                        strrms = str( "rms_PASS: %.3f " % (RMSduration_0p07cm/durationinhours) )
                    else:
                        strrms = "rms_FAIL: " + str(RMSduration_0p07cm/durationinhours)
                        strrms = str( "rms_FAIL: %.3f " % (RMSduration_0p07cm/durationinhours) )
            
                    if ( snr20_0p34cm/durationinhours < 1.0 ):
                        strnspikes = "nspikes_PASS: " + str(snr20_0p34cm/durationinhours)
                        strnspikes = str( "nspikes_PASS: %.3f " % (snr20_0p34cm/durationinhours) )
                    else:
                        strnspikes = "nspikes_FAIL: " + str(snr20_0p34cm/durationinhours)
                        strnspikes = str( "nspikes_FAIL: %.3f " % (snr20_0p34cm/durationinhours) )
            
                    if ( pctavailable > 95. ):
                        strpctavail = str( "pctavailable_PASS: %.4f " % (100*dt*nptstotal/endminusstarttime) )
                    else:
                        strpctavail = str( "pctavailable_FAIL: %.4f " % (100*dt*nptstotal/endminusstarttime) )
            
            #        print ( "%s  download: %.2fs calculate: %.2fs plot: %.2fs %s %s %s %s  Nsegments: %d  segmentlong: %.3f  segmentshort: %.3f " % (nslc, T1-T0, T2-T1-Tplot, Tplot, strngap, strrms, strnspikes, strpctavail, itrace, segmentlong, segmentshort) )
                    print ( "%s, calculate: %.2fs, plot: %.2fs, %s, %s, %s, %s,  Nsegments: %d,  segmentlong: %.3f,  segmentshort: %.3f " % (nslc, T3-T2-Tplot, Tplot, strngap, strrms, strnspikes, strpctavail, ntr, segmentlong, segmentshort) )
                    metric_thresholds = { 
                                "spikes_per_hour_threshold": 1.0, \
                                "rms_exceeded_per_hour_threshold": 60.0, \
                                "clock_lock_lcq_threshold" : None, \
                                "clock_lock_lce_threshold" : 5000
                               }
                    metrics = { 
                               "spikes_total": (snr20_0p34cm, "no_threshold"), \
                               "spikes_per_hour": (snr20_0p34cm/durationinhours, "pass" if snr20_0p34cm/durationinhours < 1.0 else "fail"), \
                               "rms_exceeded_per_hour": (RMSduration_0p07cm/durationinhours, "pass" if RMSduration_0p07cm/durationinhours < 60.0 else "fail"), \
                               }
                    if nslc not in channel_metrics:
                        channel_metrics[nslc] = metrics

            # add latency, gaps, completeness derived from sniffwave, if requested.
            latency_metrics = OrderedDict()
            for scnl in channel_metrics:
                if all_latency_metrics and scnl in all_latency_metrics:
                    l = all_latency_metrics[scnl]
                    latency_metrics[scnl] = {}
                    latency_metrics[scnl]["measurement_start"] = l["measurement_start"].replace(microsecond=0)
                    latency_metrics[scnl]["measurement_end"] = l["measurement_end"].replace(microsecond=0)
                    latency_metrics[scnl]["window_length"] = datetime.timedelta(seconds=l["data_timewindow_length"])
                    latency_metrics[scnl]["acceptable_latency"] = \
                        (l["percent_latency_good"], "pass" if l["percent_latency_good"] >= 98 else "fail")
                    latency_metrics[scnl]["gaps_per_hour"] = \
                        (l["gaps_per_hour"], "pass" if l["gaps_per_hour"] < 1 else "fail")
                    latency_metrics[scnl]["completeness"] = \
                        (l["percent_completeness"], "pass" if l["percent_completeness"] >= THRESHOLD['percent_completeness'] else "fail")
                    latency_metrics[scnl]["completeness_incl_penalty"] = \
                        (l["percent_completeness_w_penalty"], "pass" if l["percent_completeness_w_penalty"] >= THRESHOLD['percent_completeness'] else "fail")

            with open(report_dir + net_obj.code + "." + sta_obj.code + "." + DateLabel + ".html", "w") as fh:
                fh.write(template.render(starttime=starttime,endtime=endtime,institution_code=institution, \
                         network_code=net_obj.code,station=sta_obj,allowed=SEISMIC_CHANNELS, \
                         clock_metrics=soh_metrics, latency_metrics=latency_metrics, metrics=channel_metrics, \
			 nowtime=nowtime, email=email ))
            HTML(report_dir + net_obj.code + "." + sta_obj.code + "." + DateLabel +".html").write_pdf(report_dir + net_obj.code + "." + sta_obj.code + "." + DateLabel + ".pdf")
            # uncomment/include the css bootstrap in the .html file
            htmlfile = report_dir + net_obj.code + "." + sta_obj.code + "." + DateLabel + ".html"
            line1 = '<!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">  -->'
            line2 = '<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">'
            for line in fileinput.input(htmlfile, inplace = True):
               if ( "bootstrapcdn" in line and "link rel" in line ):
                   line = line.replace(line1,line2)
               print (line)

            print ("Latency info: ")
            print (str(latency_metrics))

# Remove lockfile
os.remove(pidfile)

