#!/usr/bin/env python

# Calculate some station metrics - noise floor, power at different frequencies, Nspikes...

# import std packages
import sys
from collections import Counter
import datetime
import timeit

# import third party packages
import numpy as np
from obspy.clients.fdsn import Client
from obspy import UTCDateTime
from obspy.signal.trigger import classic_sta_lta
from obspy import Stream
try:
    from ConfigParser import SafeConfigParser
except:
    from configparser import SafeConfigParser

from jinja2 import Environment, FileSystemLoader, select_autoescape
from weasyprint import HTML

# import station_metrics packages
from station_metrics.io.get_data_metadata import download_fdsn_bulk
from station_metrics.io.get_data_metadata import download_metadata_fdsn

from station_metrics.metrics.preprocessing import raw_trace_to_ground_motion_filtered_pruned
from station_metrics.metrics.noise_metrics import noise_floor, count_peaks_stalta
from station_metrics.metrics.noise_metrics import duration_exceed_RMS

from station_metrics.plotting.plot_eew_stationreport import make_station_figure_eew_stationreport

from config.parse_and_validate_args import validate_args_and_get_times, parse_args

# define which channel codes denote acceptable seismic channels
SEISMIC_CHANNELS = ['BHE', 'BHN', 'BHZ', 'HHE', 'HHN', 'HHZ',
                    'BH1', 'BH2', 'BH3', 'HH1', 'HH2', 'HH3',
                    'ENE', 'ENN', 'ENZ', 'HNE', 'HNN', 'HNZ',
                    'EN1', 'EN2', 'EN3', 'HN1', 'HN2', 'HN3']
# define a few SOH channels we might be interested in
SOH_CHANNELS = ['LCQ', 'LCE']

THRESHOLD = { 
             'lcq':60,               # minimum quality=60, read Q330 manual
             'lce':5000,             # maximum phase(drift)=5000 microseconds
             'latency':3.5,          # maximum latency 3.5s between now and middle of packet
             'rms_noise':0.0007,     # maximum RMS amplitude 0.07 cm/s^2 
             'spikes' : 0.0034       # maximum spike amplitude 0.34 sm/s^2
            }

if __name__ == "__main__":
    """
        Generates Phase1 Station Acceptance Report.
        Requirements:
            config/config.eew_stationreport --> configuring the metrics
            command-line arguments --> specify station and time period
    """
    # prepare report engine (jinja2)
    env = Environment(
        loader=FileSystemLoader("./templates"),
        autoescape=select_autoescape(['html', 'htm', 'xml'])
    )
    template = env.get_template('phase1.htm')
        
    #----- Read Processing parameters from a config file
    
    parser = SafeConfigParser()
    parser.read("config/config.eew_stationreport")
    
    FDSNtimeout = float(parser.get('SectionOne','FDSNtimeout'))
    tpadding = float(parser.get('SectionOne','tpadding'))
    sta = float(parser.get('SectionOne','sta'))
    lta = float(parser.get('SectionOne','lta'))
    tbuffer = sta + lta
    freqBP1 = float(parser.get('SectionOne','freqBP1'))
    freqBP2 = float(parser.get('SectionOne','freqBP2'))
    freqBP3 = float(parser.get('SectionOne','freqBP3'))
    freqBP4 = float(parser.get('SectionOne','freqBP4'))
    freqHP = float(parser.get('SectionOne','freqHP'))
    mpd = float(parser.get('SectionOne','mpd'))
    minSTALTAlen = float(parser.get('SectionOne','minSTALTAlen'))
    RMSlen = float(parser.get('SectionOne','RMSlen'))
    GainOrFullResp = parser.get('SectionOne','GainOrFullResp')
    #PSDperiods = (parser.get('SectionOne','PSDperiods'))[1:-1].split(",")
    #PSDperiods = [float(i) for i in PSDperiods]

    #----- command-line arguments specify what data to get and where
    # options that influence program structure:
    # infile: 
    #     multiple channels, possibly from multiple stations, are defined in an input file. 
    # command-line arguments -N network, -S station:
    #     eew_stationreport is run for the channels of a single station only.
    
    args = parse_args()
    [starttime, endtime, durationinhours, durationinsec] = validate_args_and_get_times(args)

    # get data from a little before starttime to remedy edge effects
    request_starttime = starttime - datetime.timedelta(0,lta+tpadding)
    duration_padded = durationinsec + sta + lta + (2*tpadding)
    request_endtime = starttime + datetime.timedelta(seconds=duration_padded)

    slice_starttime = starttime - datetime.timedelta(seconds=lta)
    slice_endtime = starttime + datetime.timedelta(seconds=(durationinsec + sta))

    endminusstarttime = (endtime - starttime).total_seconds()
    if ( endminusstarttime < minSTALTAlen ):
        print ("EXITING: requested time is too short or wrong " + str(starttime) + " to " + str(endtime) )
        sys.exit()

    infile = args.infile
    if infile:
        network = None
        station = None
    else: 
        network = args.network
        station = args.station
    #channel = args.channel
    #location = args.location
    datacenter = args.datacenter
    iplot = args.iplot
    
    #----- Set up the FDSN client
    
    Tdc0 = timeit.default_timer()
    try:
        client = Client(datacenter,timeout = FDSNtimeout)
    except Exception as error:
        print ("Error: {}, Failed to connect to FDSN client. Used a timeout of {}".format(error, FDSNtimeout))
    Tdc1 = timeit.default_timer()
    print ("Time to connect to FDSNWS: " + str(Tdc1-Tdc0) )
    
    #----- Download meta-data
    timer_start = timeit.default_timer()
    if infile:
        inv = download_fdsn_bulk(infile,request_starttime,duration_padded,client,type="metadata")
    else:
        inv = download_metadata_fdsn(client,net=network,sta=station,starttime=request_starttime)
    timer_end = timeit.default_timer()
    print(inv)
    inv.write("inventory.xml","STATIONXML")
    print ("Time to download metadata from {}: {}".format(datacenter, str(timer_end - timer_start)))

    # Create one report per station in the Inventory file.
    for net_obj in inv:
        network = net_obj.code
        for sta_obj in net_obj:
            station = sta_obj.code
            bulkrequest = []
            soh_bulkrequest = []

    
            # Download waveform for this station 
    
            # construct the bulkrequest list of tuples
            # single network, single station, loop over channels
            for chan in sta_obj:
                channel = chan.code
                location = chan.location_code
                if location == "":
                    location = "--"
                if channel in SEISMIC_CHANNELS:
                    bulkrequest.append( \
                        (network, station, location, channel, request_starttime.isoformat(), \
                        request_endtime.isoformat()) \
                        )
                if channel in SOH_CHANNELS:
                    soh_bulkrequest.append( \
                        (network, station, location, channel, request_starttime.isoformat(), \
                        request_endtime.isoformat()) \
                        )

            timer_start = timeit.default_timer()
            try:
                stAll = client.get_waveforms_bulk(bulkrequest)
            except Exception as e:
                print("Error: Problem requesting waveform data: {}".format(e))
                # create empty stream
                stAll = Stream()
            timer_end = timeit.default_timer()
            print ("Time to download waveforms from {}: {}".format(datacenter, str(timer_end - timer_start)))

            # Now request the SOH channels.
            soh_metrics = {}
            timer_start = timeit.default_timer()
            try:
                stSOH = client.get_waveforms_bulk(soh_bulkrequest)
            except Exception as e:
                print("Error: Problem requesting SOH data: {}".format(e))
                # create empty stream
                stSOH = Stream()
                soh_metrics["LCQ"] = {"clock_lock_lcq" : ("Not available","fail")}
                soh_metrics["LCE"] = {"clock_phase_lce": ("Not available","fail")}
 
            timer_end = timeit.default_timer()
            print ("Time to download SOH time series from {}: {}".format(datacenter, str(timer_end - timer_start)))
            
            print("Creating report for time period {} - {}, station: {}.{}".format(starttime,endtime,net_obj.code,sta_obj.code))
            # if there channels in stSOH then calculate the metric
            lcq_pass = 0
            lce_pass = 0
            number_of_lcq_points = 0
            number_of_lce_points = 0

            # anticipate possible gaps in these data but do not keep track
            for trace in stSOH:
                location = trace.stats.location
                if location == "":
                    location = "--"
                nslc = ".".join([trace.stats.network, trace.stats.station, location, trace.stats.channel])
                if trace.stats.channel == "LCQ":
                    # LCQ channel has distinct values between 0-100, e.g. 20, 60, 100
                    lcq_pass = lcq_pass + (trace.data >= THRESHOLD['lcq']).sum()
                    number_of_lcq_points = number_of_lcq_points + len(trace.data)
                if trace.stats.channel == "LCE":
                    # LCE channel can have any value, loop over all samples
                    lce_pass = lce_pass + (abs(trace.data) < THRESHOLD['lce']).sum()
                    number_of_lce_points = number_of_lce_points + len(trace.data)

            if number_of_lcq_points > 0:
                lcq_metric = 100*(lcq_pass/number_of_lcq_points)
            else:
                lcq_metric = 0
            print("Number of LCQ samples at 60 or better: {} out of {}".format(lcq_pass, \
                   number_of_lcq_points))
            soh_metrics["LCQ"] = { "clock_lock_lcq" : 
                                  (lcq_metric, "pass" if lcq_metric >= 98.0 else "fail")
                                        } 
            if number_of_lce_points > 0:
                lce_metric = 100*(lce_pass/number_of_lce_points)
            else:
                lce_metric = 0
            print("Number of LCE samples below 5000 microsecs: {} out of {}".format(lce_pass, \
                   number_of_lce_points))

            soh_metrics["LCE"] = { "clock_phase_lce" : 
                                  (lce_metric, "pass" if lce_metric >= 98.0 else "fail")
                                        } 

            #----- Loop over seismograms.  If a station has gaps, a stream of that sncl will 
            #----- be made from all the traces.
            
            # This assumes the data are in the stream ordered by net,sta,loc,chan,starttime, 
            # which is OK for the DMC (per Chad) but is NOT guaranteed by the FDSN dataselect 
            # web service standard. 
            channel_metrics = {}
            itrace = 0
            Tsum = 0
            for i in range(0,len(stAll)):
                ntr = 1
                iduplicate = 0
                if ( stAll[i].stats.location == "" ):
                    sncl = stAll[i].stats.network + "." + stAll[i].stats.station + ".--." + stAll[i].stats.channel
                else:
                    sncl = stAll[i].stats.network + "." + stAll[i].stats.station + "." + stAll[i].stats.location + "." + stAll[i].stats.channel
            #    print ( str(i) + " SNCL " + sncl )
                for j in range(0,len(stAll)):
                    if ( stAll[j].stats.location == "" ):
                        sncl2 = stAll[j].stats.network + "." + stAll[j].stats.station + ".--." + stAll[j].stats.channel
                    else:
                        sncl2 = stAll[j].stats.network + "." + stAll[j].stats.station + "." + stAll[j].stats.location + "." + stAll[j].stats.channel
                    if ( sncl == sncl2 and i != j ):
                        if ( j < i ):
                            iduplicate = 1
                            break
                        else:
                            ntr = ntr + 1
                if ( iduplicate == 0 ):
                    T2 = timeit.default_timer()
                    itrace = itrace + 1
                    tr = []
                    trRaw = []
                    trVelHighPass = []
                    trAccFilt = []
                    trVelFilt = []
                    stalta = []
                    dataraw = []
                    dataVelHighPass = []
                    dataAccFilt = []
                    dataVelFilt = []
                    datastalta = []
                    nptstotal = 0
                    dt = stAll[i].stats.delta
                    segmentlong = 0
                    segmentshort = 1e10
                    for j in range(i,i+ntr):
                        trRaw = stAll[j].copy()
                        trRaw = trRaw.slice(UTCDateTime(slice_starttime),UTCDateTime(slice_endtime))
                        npts = trRaw.stats.npts
                        trPadded = stAll[j].copy()
                        trPadded = trPadded.detrend(type='polynomial',order=3)

                        trVelHighPass = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                     slice_starttime, slice_endtime, "Vel", 0, 0, \
                                     freqHP, 0, 0, inv)
                        trAccFilt = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                    slice_starttime, slice_endtime, "Acc", 0, freqBP1, \
                                    freqBP2, freqBP3, freqBP4, inv)
                        trVelFilt = raw_trace_to_ground_motion_filtered_pruned(trPadded, \
                                    slice_starttime, slice_endtime, "Vel", 0, freqBP1, \
                                    freqBP2, freqBP3, freqBP4, inv)
                        stalta = trVelHighPass.copy()
                        dataraw = np.concatenate((dataraw,trRaw.data),axis=0)
                        dataVelHighPass = np.concatenate((dataVelHighPass,trVelHighPass.data),axis=0)
                        dataAccFilt = np.concatenate((dataAccFilt,trAccFilt.data),axis=0)
                        dataVelFilt = np.concatenate((dataVelFilt,trVelFilt.data),axis=0)

                        if ( npts*dt > minSTALTAlen ):
            #                stalta = z_detect(trVelHighPass,int(1.0/dt))                      #--- of order 0.1 sec/trace 
                            stalta = classic_sta_lta(trVelHighPass,int(sta/dt),int(lta/dt))  #--- of order 0.4 sec/trace
                            datastalta = np.concatenate((datastalta,stalta),axis=0)
                        else:
                            datastalta = np.concatenate((datastalta,np.zeros(trVelHighPass.stats.npts)),axis=0)
                        T4 = timeit.default_timer()
                        if ( iplot == 1 ):
                            if ( j == i ):
                                strawplot = Stream(traces=[trRaw])
                                stVelHighPassplot = Stream(traces=[trVelHighPass])
                                stAccFiltplot = Stream(traces=[trAccFilt])
                            else:
                                strawplot += Stream(traces=[trRaw])
                                stVelHighPassplot += Stream(traces=[trVelHighPass])
                                stAccFiltplot += Stream(traces=[trAccFilt])
                            if ( j == i + ntr -1 ):
                                AccRMSlabel = str(freqBP2) + "-" + str(freqBP3) + "Hz RMS Acc (cm/s^2)"
                                AccNspikeslabel =  str(freqBP2) + "-" + str(freqBP3) + "Hz Spikes Acc (cm/s^2)"
                                VelHighPasslabel =  "   HighPass " + str(freqHP) + "Hz    Vel (cm/s)"
                                for ij in range(0,len(stVelHighPassplot)):
                                    stVelHighPassplot[ij].data = stVelHighPassplot[ij].data*100
                                    stAccFiltplot[ij].data = stAccFiltplot[ij].data*100
#                                    stVelFiltplot[ij].data = stVelFiltplot[ij].data*100
                                make_station_figure_eew_stationreport(strawplot,stVelHighPassplot,stAccFiltplot,stAccFiltplot,datastalta,"Raw       (counts)",VelHighPasslabel,AccRMSlabel,AccNspikeslabel,"     STA/LTA",0,0,0.07,0.34,20,RMSlen)  #--- of order 25sec/channel for 1 hr long traces
                                stVelHighPassplot.clear()
                                stAccFiltplot.clear()
                        T5 = timeit.default_timer()
                        Tplot = T5-T4
                        print (" " )
                        print ("NPTS 1:  " + str(npts) + " " + str(nptstotal) )
                        trRaw = trRaw.slice(UTCDateTime(starttime),UTCDateTime(endtime))
                        npts = trRaw.stats.npts
                        nptstotal = nptstotal + npts
                        print ("NPTS 2:  " + str(npts) + " " + str(nptstotal) )
                        if ( dt*npts > segmentlong ):
                            segmentlong = dt*npts
                        if ( dt*npts < segmentshort ):
                            segmentshort = dt*npts

                    rawrange = float(np.ptp(dataraw))
                    rawmean = np.mean(dataraw)
            #        rawrms = np.sqrt(np.vdot(dataraw, dataraw)/dataraw.size)
                    rawrms = np.sqrt(np.mean((dataraw-rawmean)**2))
                    rawmin = min(dataraw)
                    rawmax = max(dataraw)
                    pctavailable = 100.*(nptstotal/(endminusstarttime/dt))
                    ngaps = ntr - 1
                    NoiseFloorAcc = noise_floor(dataAccFilt)
                    NoiseFloorVel = noise_floor(dataVelFilt)
                    dataVelFilt = []
                    snr20_0p34cm = count_peaks_stalta(dataAccFilt,datastalta,sta,lta,mpd,20,dt,THRESHOLD['spikes'])  #--- ShakeAlert stat. acceptance thresh. is now 0.0034 m/s^2 = 0.34cm/s^2.
                    datastalta = []
                    RMSduration_0p07cm = duration_exceed_RMS(dataAccFilt,THRESHOLD['rms_noise'],RMSlen,dt)  #--- ShakeAlert stat. acceptance thresh. is now 0.0007 m/s^2 = 0.07cm/s^2.
                    dataAccFilt = []
                    T3 = timeit.default_timer()
            
                    if ( ngaps/durationinhours < 1.0 ):
                        strngap = "ngaps_PASS: " + str(ngaps/durationinhours)
                        strngap = str( "ngaps_PASS: %.3f " % (ngaps/durationinhours) )
                    else:
                        strngap = "ngaps_FAIL: " + str(ngaps/durationinhours)
                        strngap = str( "ngaps_FAIL: %.3f " % (ngaps/durationinhours) )
            
                    if ( RMSduration_0p07cm/durationinhours < 60. ):
                        strrms = "rms_PASS: " + str(RMSduration_0p07cm/durationinhours)
                        strrms = str( "rms_PASS: %.3f " % (RMSduration_0p07cm/durationinhours) )
                    else:
                        strrms = "rms_FAIL: " + str(RMSduration_0p07cm/durationinhours)
                        strrms = str( "rms_FAIL: %.3f " % (RMSduration_0p07cm/durationinhours) )
            
                    if ( snr20_0p34cm/durationinhours < 1.0 ):
                        strnspikes = "nspikes_PASS: " + str(snr20_0p34cm/durationinhours)
                        strnspikes = str( "nspikes_PASS: %.3f " % (snr20_0p34cm/durationinhours) )
                    else:
                        strnspikes = "nspikes_FAIL: " + str(snr20_0p34cm/durationinhours)
                        strnspikes = str( "nspikes_FAIL: %.3f " % (snr20_0p34cm/durationinhours) )
            
                    if ( pctavailable > 95. ):
                        strpctavail = str( "pctavailable_PASS: %.4f " % (100*dt*nptstotal/endminusstarttime) )
                    else:
                        strpctavail = str( "pctavailable_FAIL: %.4f " % (100*dt*nptstotal/endminusstarttime) )
            
            #        print ( "%s  download: %.2fs calculate: %.2fs plot: %.2fs %s %s %s %s  Nsegments: %d  segmentlong: %.3f  segmentshort: %.3f " % (sncl, T1-T0, T2-T1-Tplot, Tplot, strngap, strrms, strnspikes, strpctavail, itrace, segmentlong, segmentshort) )
                    print ( "%s, calculate: %.2fs, plot: %.2fs, %s, %s, %s, %s,  Nsegments: %d,  segmentlong: %.3f,  segmentshort: %.3f " % (sncl, T3-T2-Tplot, Tplot, strngap, strrms, strnspikes, strpctavail, ntr, segmentlong, segmentshort) )
                    metric_thresholds = { 
                                "spikes_per_hour_threshold": 1.0, \
                                "rms_exceeded_per_hour_threshold": 60.0, \
                                "clock_lock_lcq_threshold" : None, \
                                "clock_lock_lce_threshold" : 5000
                               }
                    metrics = { 
                               "spikes_total": (snr20_0p34cm, "no_threshold"), \
                               "spikes_per_hour": (snr20_0p34cm/durationinhours, "pass" if snr20_0p34cm/durationinhours < 1.0 else "fail"), \
                               "rms_exceeded_per_hour": (RMSduration_0p07cm/durationinhours, "pass" if RMSduration_0p07cm/durationinhours < 60.0 else "fail"), \
                               "acceptable_latency" : (97.3, "fail")
                               }
                    if sncl not in channel_metrics:
                        channel_metrics[sncl] = metrics

            with open("./templates/" + net_obj.code + "." + sta_obj.code + ".html", "w") as fh:
                fh.write(template.render(starttime=starttime,endtime=endtime, \
                         network_code=net_obj.code,station=sta_obj,allowed=SEISMIC_CHANNELS, \
                         clock_metrics=soh_metrics, metrics=channel_metrics))
            HTML("./templates/"+net_obj.code + "." + sta_obj.code+".html").write_pdf(net_obj.code + "." + sta_obj.code + ".pdf")

